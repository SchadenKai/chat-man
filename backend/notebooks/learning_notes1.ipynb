{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interseciton set:  {'I', 'Here'}  which in total have:  2\n",
      "Union set:  {'how', 'am', 'Hello', 'code', 'learn', 'want', 'world', 'to', 'I', 'Here', 'Coding'}  which in total have:  11\n",
      "0.18\n"
     ]
    }
   ],
   "source": [
    "def jaccard(set_a: set[str], set_b: set[str]) -> float:\n",
    "    intersection = set_a.intersection(set_b)\n",
    "    union = set_a.union(set_b)\n",
    "    print(\"Interseciton set: \", intersection, \" which in total have: \", len(intersection))\n",
    "    print(\"Union set: \", union, \" which in total have: \", len(union))\n",
    "    return round(len(intersection) / len(union), 2)\n",
    "\n",
    "def tokenize(input: str) -> set[str]:\n",
    "    \"\"\"split the string by spaces and convert it into a set\"\"\"\n",
    "    return set(input.split())\n",
    "\n",
    "def w_shingling_tokenization(input: str) -> set[str]:\n",
    "    \"\"\"tokenize a string using w-shingling\"\"\"\n",
    "    \n",
    "\n",
    "doc1 = \"Hello world I am Coding Here\"\n",
    "doc2 = \"I want to learn how to code Here\"\n",
    "\n",
    "doc1_set = tokenize(doc1)\n",
    "doc2_set = tokenize(doc2)\n",
    "similarity_score = jaccard(doc1_set, doc2_set)\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4aa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag-ui-testing-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
